from youtube_transcript_api import YouTubeTranscriptApi
import re
from urllib.parse import urlparse, parse_qs
from pathlib import Path
import json
import csv
import time
from datetime import datetime

def extract_video_id(url: str) -> str:
    """YouTube URLì—ì„œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ"""
    # ë‹¤ì–‘í•œ YouTube URL í˜•ì‹ ì²˜ë¦¬
    patterns = [
        r'(?:youtube\.com/watch\?v=|youtu\.be/|youtube\.com/embed/)([A-Za-z0-9_-]{11})',
        r'youtube\.com/shorts/([A-Za-z0-9_-]{11})',
        r'youtube\.com/v/([A-Za-z0-9_-]{11})'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    
    # URL íŒŒì‹±ìœ¼ë¡œ ì‹œë„
    try:
        parsed = urlparse(url)
        if 'youtu.be' in parsed.netloc:
            return parsed.path.strip('/')
        elif 'youtube.com' in parsed.netloc:
            query_params = parse_qs(parsed.query)
            if 'v' in query_params:
                return query_params['v'][0]
    except:
        pass
    
    raise ValueError(f"Could not extract video ID from: {url}")

def fetch_transcript(video_id, language="ko"):
    """
    ìœ íŠœë¸Œ video_idì™€ ì–¸ì–´ì½”ë“œë¡œ ìë§‰(ìŠ¤í¬ë¦½íŠ¸) í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        transcript_list = YouTubeTranscriptApi().list(video_id)
        transcript = None
        lang_info = ""  # ê¸°ë³¸ê°’

        # ìš°ì„  í•œêµ­ì–´ ìˆ˜ë™ ìë§‰ì„ ì°¾ëŠ”ë‹¤.
        try:
            transcript = transcript_list.find_transcript([language])
            lang_info = transcript.language_code
        except Exception:
            # ì—†ìœ¼ë©´ ìë™ ìƒì„± ìë§‰ì„ ì°¾ëŠ”ë‹¤.
            try:
                transcript = transcript_list.find_generated_transcript([language])
                lang_info = f"{transcript.language_code} (auto-generated)"
            except Exception:
                # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ìë§‰ ì‚¬ìš©
                try:
                    transcript = next(iter(transcript_list))
                    lang_info = transcript.language_code  # ì˜ˆ: 'en'
                except Exception:
                    return "Error: No transcript available."
        
        # transcript.fetch() ê²°ê³¼ì—ì„œ text ì†ì„±ë§Œ ì¶”ì¶œ
        transcript_data = transcript.fetch()
        text_list = []
        for entry in transcript_data:
            if hasattr(entry, 'text'):
                if entry.text:
                    text_list.append(entry.text)
            elif isinstance(entry, dict) and 'text' in entry:
                text_list.append(entry['text'])
        
        full_text = ' '.join(text_list)
        return f"[{lang_info}] {full_text}"
    
    except Exception as e:
        return f"Error fetching transcript: {e}"

def extract_youtube_urls(file_path: Path):
    """íŒŒì¼ì—ì„œ YouTube URL ì¶”ì¶œ"""
    try:
        content = file_path.read_text(encoding='utf-8')
    except UnicodeDecodeError:
        # UTF-8ë¡œ ì•ˆë˜ë©´ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
        try:
            content = file_path.read_text(encoding='cp949')
        except:
            content = file_path.read_text(encoding='latin-1')
    
    # YouTube URL íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰
    youtube_patterns = [
        r'https?://(?:www\.)?youtube\.com/watch\?v=[A-Za-z0-9_-]{11}[^\s]*',
        r'https?://youtu\.be/[A-Za-z0-9_-]{11}[^\s]*',
        r'https?://(?:www\.)?youtube\.com/embed/[A-Za-z0-9_-]{11}[^\s]*',
        r'https?://(?:www\.)?youtube\.com/shorts/[A-Za-z0-9_-]{11}[^\s]*'
    ]
    
    urls = []
    for pattern in youtube_patterns:
        found = re.findall(pattern, content)
        urls.extend(found)
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    unique_urls = []
    seen = set()
    for url in urls:
        # URL ëì˜ ë”°ì˜´í‘œë‚˜ ê´„í˜¸ ì œê±°
        cleaned_url = re.sub(r'["\'\)]*$', '', url)
        if cleaned_url not in seen:
            seen.add(cleaned_url)
            unique_urls.append(cleaned_url)
    
    return unique_urls

def normalize_text(text: str) -> str:
    """ìë§‰ í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if text.startswith("Error"):
        return text
    
    # ì–¸ì–´ ì •ë³´ ì¶”ì¶œ
    lang_match = re.match(r'\[([^\]]*)\]\s*(.*)', text)
    if lang_match:
        lang_info = lang_match.group(1)
        content = lang_match.group(2)
    else:
        lang_info = "unknown"
        content = text
    
    # í…ìŠ¤íŠ¸ ì •ë¦¬
    content = re.sub(r'\s+', ' ', content)  # ê³µë°± ì •ë¦¬
    content = re.sub(r'https?://\S+', ' ', content)  # URL ì œê±°
    content = re.sub(r'#\w+', ' ', content)  # í•´ì‹œíƒœê·¸ ì œê±°
    content = content.strip()
    
    return f"[{lang_info}] {content}"

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì„¤ì •
    INPUT_FILE = Path("youtube_urls.txt")  # YouTube ë§í¬ê°€ ìˆëŠ” íŒŒì¼
    OUTPUT_DIR = Path("transcripts_output")  # ê²°ê³¼ ì €ì¥ í´ë”
    
    # ì¶œë ¥ í´ë” ìƒì„±
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    print(f"ğŸ“‚ {INPUT_FILE} íŒŒì¼ì—ì„œ YouTube URLì„ ì°¾ëŠ” ì¤‘...")
    
    if not INPUT_FILE.exists():
        print(f"âŒ {INPUT_FILE} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # YouTube URL ì¶”ì¶œ
    try:
        urls = extract_youtube_urls(INPUT_FILE)
        print(f"âœ… {len(urls)}ê°œì˜ YouTube URLì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        
        if not urls:
            print("âŒ YouTube URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
            
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return
    
    # ìë§‰ ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜
    all_transcripts = []
    success_count = 0
    error_count = 0
    
    print(f"\nğŸ¬ {len(urls)}ê°œ ì˜ìƒì˜ ìë§‰ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    print("="*60)
    
    for i, url in enumerate(urls, 1):
        print(f"\n[{i:2d}/{len(urls)}] ì²˜ë¦¬ ì¤‘: {url[:60]}...")
        
        try:
            # ë¹„ë””ì˜¤ ID ì¶”ì¶œ
            video_id = extract_video_id(url)
            print(f"   ğŸ“¹ Video ID: {video_id}")
            
            # ìë§‰ ì¶”ì¶œ
            transcript_text = fetch_transcript(video_id, language="ko")
            
            if transcript_text.startswith("Error"):
                print(f"   âŒ {transcript_text}")
                error_count += 1
            else:
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                cleaned_text = normalize_text(transcript_text)
                print(f"   âœ… ì„±ê³µ - ê¸¸ì´: {len(cleaned_text)} ë¬¸ì")
                print(f"   ğŸ“ ë¯¸ë¦¬ë³´ê¸°: {cleaned_text[:80]}...")
                success_count += 1
                
                # ê²°ê³¼ ì €ì¥
                transcript_data = {
                    'index': i,
                    'video_id': video_id,
                    'url': url,
                    'raw_transcript': transcript_text,
                    'cleaned_transcript': cleaned_text,
                    'character_count': len(cleaned_text),
                    'timestamp': datetime.now().isoformat()
                }
                all_transcripts.append(transcript_data)
                
                # ê°œë³„ íŒŒì¼ë¡œë„ ì €ì¥
                individual_file = OUTPUT_DIR / f"{i:02d}_{video_id}.txt"
                individual_file.write_text(cleaned_text, encoding='utf-8')
            
            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
            time.sleep(0.5)
            
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
            error_count += 1
            continue
    
    # ì „ì²´ ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # 1. JSON íŒŒì¼ë¡œ ì €ì¥
    json_file = OUTPUT_DIR / f"all_transcripts_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_transcripts, f, ensure_ascii=False, indent=2)
    
    # 2. CSV ìš”ì•½ íŒŒì¼
    csv_file = OUTPUT_DIR / f"transcripts_summary_{timestamp}.csv"
    if all_transcripts:
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['index', 'video_id', 'url', 'character_count', 'timestamp'])
            writer.writeheader()
            for transcript in all_transcripts:
                writer.writerow({
                    'index': transcript['index'],
                    'video_id': transcript['video_id'],
                    'url': transcript['url'],
                    'character_count': transcript['character_count'],
                    'timestamp': transcript['timestamp']
                })
    
    # 3. í†µí•© í…ìŠ¤íŠ¸ íŒŒì¼
    combined_file = OUTPUT_DIR / f"all_transcripts_combined_{timestamp}.txt"
    with open(combined_file, 'w', encoding='utf-8') as f:
        for transcript in all_transcripts:
            f.write(f"{'='*60}\n")
            f.write(f"[{transcript['index']:02d}] {transcript['video_id']}\n")
            f.write(f"URL: {transcript['url']}\n")
            f.write(f"{'='*60}\n")
            f.write(f"{transcript['cleaned_transcript']}\n\n")
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ğŸ‰ ìë§‰ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {error_count}ê°œ")
    print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼:")
    print(f"   ğŸ“„ JSON: {json_file.name}")
    print(f"   ğŸ“Š CSV: {csv_file.name}")
    print(f"   ğŸ“ í†µí•©: {combined_file.name}")
    print(f"   ğŸ“‚ ê°œë³„: {OUTPUT_DIR}/*.txt")
    
    # ì„±ê³µë¥  ì¶œë ¥
    if len(urls) > 0:
        success_rate = (success_count / len(urls)) * 100
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
    
    return all_transcripts

if __name__ == "__main__":
    print("ğŸš€ YouTube ëŒ€ëŸ‰ ìë§‰ ìˆ˜ì§‘ê¸° ì‹œì‘")
    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€: pip install youtube-transcript-api")
    print(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: youtube_urls.txt")
    print(f"ğŸ“ ì¶œë ¥ í´ë”: transcripts_output")
    print()
    
    # ì‹¤í–‰
    transcripts = main()
    
    if transcripts:
        print(f"\nğŸ“Š ìˆ˜ì§‘ëœ ìë§‰ ë°ì´í„°:")
        print(f"   ì´ {len(transcripts)}ê°œ ì˜ìƒ")
        
        # ì–¸ì–´ë³„ í†µê³„
        lang_stats = {}
        total_chars = 0
        for t in transcripts:
            # ì–¸ì–´ ì •ë³´ ì¶”ì¶œ
            lang_match = re.match(r'\[([^\]]*)\]', t['cleaned_transcript'])
            lang = lang_match.group(1) if lang_match else 'unknown'
            lang_stats[lang] = lang_stats.get(lang, 0) + 1
            total_chars += t['character_count']
        
        print(f"   ì´ ë¬¸ììˆ˜: {total_chars:,}ì")
        print(f"   ì–¸ì–´ë³„ ë¶„í¬:")
        for lang, count in sorted(lang_stats.items()):
            print(f"     {lang}: {count}ê°œ")
    
    print(f"\nğŸ’¡ ì‚¬ìš© íŒ:")
    print(f"   - transcripts ë³€ìˆ˜ì— ëª¨ë“  ë°ì´í„°ê°€ ì €ì¥ë¨")
    print(f"   - transcripts_output í´ë”ì—ì„œ ê²°ê³¼ íŒŒì¼ í™•ì¸")
    print(f"   - ê°œë³„ txt íŒŒì¼ë“¤ì„ í›„ì²˜ë¦¬ì— í™œìš© ê°€ëŠ¥")