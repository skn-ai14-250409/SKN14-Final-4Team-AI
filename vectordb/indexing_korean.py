# ===== 1) í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import =====
# í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •

# ===== 2) í™˜ê²½ë³€ìˆ˜ ì„¤ì • (.env íŒŒì¼ ë¡œë“œ) =====
import os, glob
from pathlib import Path

# .env íŒŒì¼ ë¡œë“œ
def load_env_file():
    env_file = Path(".env")
    if env_file.exists():
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# .env íŒŒì¼ ë¡œë“œ ì‹œë„
try:
    load_env_file()
except Exception as e:
    print(f"âš ï¸  .env íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

# Colab userdata ì‹œë„ (Colab í™˜ê²½ì—ì„œë§Œ)
try:
    from google.colab import userdata
    os.environ["OPENAI_API_KEY"]   = userdata.get("FINAL_OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY","")
    os.environ["PINECONE_API_KEY"] = userdata.get("PINECONE_API_KEY") or os.getenv("PINECONE_API_KEY","")
    # LangSmith (ì„ íƒ)
    for k in ["LANGSMITH_TRACING","LANGSMITH_ENDPOINT","LANGSMITH_API_KEY","LANGSMITH_PROJECT"]:
        v = userdata.get(k) or os.getenv(k)
        if v: os.environ[k] = v
except Exception:
    pass

assert os.environ.get("OPENAI_API_KEY"),   "âŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤."
assert os.environ.get("PINECONE_API_KEY"), "âŒ PINECONE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤."

# ===== 3) ë¡œì»¬ ì´ë¦„ ì¶©ëŒ ë°©ì§€ (./pinecone.py ë“±) =====
_conflicts = [p for p in glob.glob("./pinecone*") if Path(p).exists()]
if _conflicts:
    raise RuntimeError(f"âŒ ì‘ì—… í´ë” ì´ë¦„ ì¶©ëŒ: {_conflicts}  (ì´ë¦„ ë³€ê²½ í›„ ì¬ì‹¤í–‰)")

# ===== 4) ì„í¬íŠ¸ =====
from pinecone import Pinecone, ServerlessSpec
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
print("OK: imports")

# ===== ì„¤ì • (í•œêµ­ì–´ ìµœì í™”) =====
INDEX_NAME   = "transcripts-korean"
NAMESPACE    = "transcripts-kr"
CLOUD, REGION = "aws", "us-east-1"
METRIC       = "cosine"

# í•œêµ­ì–´ì— ë” ì í•©í•œ ëª¨ë¸ ì„¤ì •
OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"  # í•œêµ­ì–´ ì„±ëŠ¥ì´ ë” ì¢‹ìŒ
OPENAI_LLM_MODEL       = "gpt-4o-mini"             # í•œêµ­ì–´ ì§€ì›

MODEL_DIMS = {"text-embedding-3-small": 1536, "text-embedding-3-large": 3072}
EMBED_DIM  = MODEL_DIMS[OPENAI_EMBEDDING_MODEL]

# ===== í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ & ì²­í¬ =====
import re, uuid

def clean_text(t: str) -> str:
    # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ë¦¬
    t = re.sub(r"[ \t]+\n", "\n", t)
    t = re.sub(r"\n{3,}", "\n\n", t).strip()
    t = t.replace("â€“", "-").replace("â€”", "-")
    # í•œêµ­ì–´ íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
    t = re.sub(r"\[ìŒì•…\]", "", t)  # [ìŒì•…] ì œê±°
    t = re.sub(r"\[.*?\]", "", t)   # ëŒ€ê´„í˜¸ ë‚´ìš© ì œê±°
    t = re.sub(r"\s+", " ", t)      # ì—°ì† ê³µë°± ì •ë¦¬
    return t.strip()

def split_sections(t: str):
    # YouTube ìë§‰ íŒŒì¼ì˜ ê²½ìš° ì—°ì†ëœ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ General ì„¹ì…˜ìœ¼ë¡œ ì²˜ë¦¬
    # íŒ¨ì…˜ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ì„¹ì…˜ì„ ìë™ ë¶„ë¥˜
    text_lower = t.lower()
    
    # íŒ¨ì…˜ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ì„¹ì…˜ ë¶„ë¥˜
    if any(keyword in text_lower for keyword in ['ì¶œê·¼', 'ì§ì¥', 'ì˜¤í”¼ìŠ¤', 'ì—…ë¬´', 'íšŒì‚¬']):
        section = "ì¶œê·¼ë£©"
    elif any(keyword in text_lower for keyword in ['ë°ì´íŠ¸', 'ì—°ì• ', 'ë‚¨ì¹œ', 'ì—¬ì¹œ', 'ì»¤í”Œ']):
        section = "ë°ì´íŠ¸ë£©"
    elif any(keyword in text_lower for keyword in ['ìºì£¼ì–¼', 'ì¼ìƒ', 'ë°ì¼ë¦¬', 'í¸í•œ']):
        section = "ìºì£¼ì–¼ë£©"
    elif any(keyword in text_lower for keyword in ['ë¯¸ë‹ˆë©€', 'ì‹¬í”Œ', 'ê¹”ë”', 'ë² ì´ì§']):
        section = "ë¯¸ë‹ˆë©€ë£©"
    elif any(keyword in text_lower for keyword in ['ì—¬ë¦„', 'summer', 'ì‹œì›í•œ', 'ê°€ë²¼ìš´']):
        section = "ì—¬ë¦„ë£©"
    elif any(keyword in text_lower for keyword in ['ê²¨ìš¸', 'winter', 'ë”°ëœ»í•œ', 'ë³´ì˜¨']):
        section = "ê²¨ìš¸ë£©"
    else:
        section = "General"
    
    return [{"section": section, "text": t}]

def chunk_by_paragraph(t, max_chars=1600, overlap_chars=200):
    # YouTube ìë§‰ì€ ì—°ì†ëœ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
    # ë¬¸ì¥ êµ¬ë¶„ì: ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ
    sentences = re.split(r'[.!?]+', t)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    chunks, cur = [], ""
    for sentence in sentences:
        # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œì˜ ê¸¸ì´
        potential_chunk = (cur + " " + sentence).strip() if cur else sentence
        
        if len(potential_chunk) <= max_chars:
            cur = potential_chunk
        else:
            # í˜„ì¬ ì²­í¬ê°€ ìˆìœ¼ë©´ ì €ì¥
            if cur:
                chunks.append(cur)
            # ìƒˆ ì²­í¬ ì‹œì‘
            cur = sentence
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if cur:
        chunks.append(cur)
    
    # ì²­í¬ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
    if not chunks:
        chunks = [t[:max_chars]] if len(t) > max_chars else [t]
    
    # ì˜¤ë²„ë© ì²˜ë¦¬
    if overlap_chars and len(chunks) > 1:
        for k in range(1, len(chunks)):
            prev = chunks[k-1]
            if len(prev) > overlap_chars:
                overlap = prev[-overlap_chars:]
                chunks[k] = overlap + " " + chunks[k]
    
    return chunks

# ===== occasion ìë™ íƒœê¹… =====
def infer_occasion(text: str):
    t = text.lower()
    tags = set()
    if re.search(r"\b(office|work|commute|promotion|slacks|shirt|tweed|mary jane)\b", t):
        tags.add("work")
    if re.search(r"\b(festival|concert)\b", t):
        tags.add("festival")
    if re.search(r"\b(travel|airport)\b", t):
        tags.add("travel")
    if re.search(r"\b(casual|weekend|hobo bag)\b", t):
        tags.add("casual")
    return list(tags or {"general"})

    # ===== Pinecone ì´ˆê¸°í™” & ì¸ë±ìŠ¤ ìƒì„±/ì ‘ì† =====
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])
if INDEX_NAME not in {i.name for i in pc.list_indexes()}:
    pc.create_index(
        name=INDEX_NAME, dimension=EMBED_DIM, metric=METRIC,
        spec=ServerlessSpec(cloud=CLOUD, region=REGION),
    )
    print(f"âœ… Created index: {INDEX_NAME}")
index = pc.Index(INDEX_NAME)

# ===== ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ =====
emb = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL)
print("OK: Pinecone & Embeddings ready")

from pathlib import Path

def ingest_files(file_paths, *, namespace=NAMESPACE, add_snippet=True):
    total_vectors, doc_map = 0, {}
    for f in file_paths:
        path = Path(f)
        assert path.exists(), f"âŒ íŒŒì¼ ì—†ìŒ: {path.resolve()}"

        text = clean_text(path.read_text(encoding="utf-8"))
        docs = split_sections(text)
        
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {path.name} - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}, ì„¹ì…˜ ìˆ˜: {len(docs)}")

        # ë ˆì½”ë“œ êµ¬ì„±(ìŠ¤ë‹ˆí«/occasion ë©”íƒ€ í¬í•¨)
        records = []
        for d in docs:
            chunks = chunk_by_paragraph(d["text"], max_chars=1600, overlap_chars=200)
            print(f"  ì„¹ì…˜: {d['section']} - ì²­í¬ ìˆ˜: {len(chunks)}")
            for ch in chunks:
                md = {
                    "section": d["section"],
                    "season": "summer",
                    "exposure": "non_revealing",
                    "source": str(path),
                    "occasion": infer_occasion(ch)
                }
                if add_snippet:
                    md["snippet"] = ch[:220].replace("\n", " ")
                records.append({"text": f"Section: {d['section']}\n\n{ch}", "metadata": md})

        # ì—…ì„œíŠ¸
        if not records:
            print(f"  âš ï¸  {path.name}: ë ˆì½”ë“œê°€ ì—†ì–´ì„œ ê±´ë„ˆëœ€")
            continue
        doc_id = str(uuid.uuid4())
        texts  = [r["text"] for r in records]
        metas  = [r["metadata"] for r in records]
        ids    = [f"{doc_id}-{i}" for i in range(len(records))]

        vectors = []
        BATCH = 64
        for s in range(0, len(texts), BATCH):
            embs = emb.embed_documents(texts[s:s+BATCH])
            for j, vec in enumerate(embs):
                k = s + j
                vectors.append({
                    "id": ids[k],
                    "values": vec,
                    "metadata": {**metas[k], "doc_id": doc_id, "chunk_id": k, "lang": "en"},
                })

        index.upsert(vectors=vectors, namespace=namespace)
        total_vectors += len(vectors)
        doc_map[doc_id] = str(path.name)
        print(f"âœ… Upserted {len(vectors)} vectors from '{path.name}' (ns='{namespace}')")
    print(f"ğŸ¯ Done. total_vectors={total_vectors}, files={len(file_paths)}, namespace='{namespace}'")
    return {"total_vectors": total_vectors, "doc_map": doc_map}

# transcripts_output í´ë”ì˜ ëª¨ë“  ê°œë³„ í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ë™ì ìœ¼ë¡œ ì°¾ê¸°
import glob
transcripts_dir = "../style_rules/transcripts_output"
transcript_files_with_path = []

# ê°œë³„ ìë§‰ íŒŒì¼ë“¤ë§Œ ì°¾ê¸° (ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ë“¤)
pattern = f"{transcripts_dir}/*.txt"
all_files = glob.glob(pattern)

for file_path in all_files:
    filename = Path(file_path).name
    # ê°œë³„ ìë§‰ íŒŒì¼ë§Œ ì„ íƒ (ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ë“¤)
    if filename.startswith(('01_', '02_', '03_', '04_', '05_', '06_', '07_', '08_', '09_', '10_', 
                           '11_', '12_', '13_', '14_', '15_', '16_', '17_', '18_', '19_', '20_',
                           '21_', '22_', '23_', '24_', '25_', '26_', '27_', '28_', '29_', '30_',
                           '31_', '32_', '33_', '34_', '35_', '36_', '37_', '38_', '39_', '40_')):
        transcript_files_with_path.append(file_path)

# íŒŒì¼ ì •ë ¬
transcript_files_with_path.sort()

print(f"ğŸ“ ë°œê²¬ëœ ìë§‰ íŒŒì¼ ìˆ˜: {len(transcript_files_with_path)}")
for i, file_path in enumerate(transcript_files_with_path[:5], 1):
    print(f"  {i}. {Path(file_path).name}")
if len(transcript_files_with_path) > 5:
    print(f"  ... ë° {len(transcript_files_with_path) - 5}ê°œ ë”")

result = ingest_files(transcript_files_with_path)
result

def pinecone_search(query_text, k=5, season="summer", occasion=None, section=None, namespace=NAMESPACE):
    qvec = emb.embed_query(query_text)
    _filter = {}
    if season:   _filter["season"] = {"$eq": season}
    if occasion: _filter["occasion"] = {"$eq": occasion}
    if section:  _filter["section"] = {"$eq": section}

    res = index.query(
        vector=qvec, top_k=k, include_values=False, include_metadata=True,
        namespace=namespace, filter=_filter or None
    )
    matches = res.get("matches", []) or []
    for i, m in enumerate(matches, 1):
        md = m.get("metadata", {}) or {}
        print(f"\n[{i}] score={m.get('score'):.4f}  section={md.get('section')}  occasion={md.get('occasion')} source={md.get('source')}")
        if "snippet" in md:
            print("snippet:", md["snippet"])

def korean_search(query_text, k=5, namespace=NAMESPACE):
    """í•œêµ­ì–´ íŒ¨ì…˜ ê²€ìƒ‰ í•¨ìˆ˜"""
    print(f"ğŸ” í•œêµ­ì–´ ê²€ìƒ‰: '{query_text}'")
    qvec = emb.embed_query(query_text)
    
    res = index.query(
        vector=qvec, top_k=k, include_values=False, include_metadata=True,
        namespace=namespace
    )
    
    matches = res.get("matches", []) or []
    print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(matches)}ê°œ")
    
    for i, m in enumerate(matches, 1):
        md = m.get("metadata", {}) or {}
        print(f"\n[{i}] ìœ ì‚¬ë„: {m.get('score'):.4f}")
        print(f"    ì„¹ì…˜: {md.get('section', 'N/A')}")
        print(f"    ìƒí™©: {md.get('occasion', 'N/A')}")
        print(f"    ì¶œì²˜: {md.get('source', 'N/A')}")
        if "snippet" in md:
            print(f"    ë‚´ìš©: {md['snippet'][:100]}...")
    
    return matches

def build_context_from_matches(res, max_chunks=3, max_chars=800):
    chunks = []
    for m in (res.get("matches") or [])[:max_chunks]:
        md = m.get("metadata", {}) or {}
        body = md.get("snippet","")
        title = md.get("section","")
        chunks.append(f"[{title}] {body[:max_chars]}")
    return "\n\n".join(chunks)

# ì˜ˆì‹œ: í•œêµ­ì–´ íŒ¨ì…˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
print("\n=== í•œêµ­ì–´ íŒ¨ì…˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ===")
korean_search("ì—¬ë¦„ ì¶œê·¼ë£©", k=3)

# ê¸°ì¡´ ê²€ìƒ‰ë„ í…ŒìŠ¤íŠ¸
print("\n=== ê¸°ì¡´ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ===")
res = pinecone_search("30ëŒ€ ì—¬ì ì—¬ë¦„ ì¶œê·¼ë£©", k=5, occasion="work")
if res:
    docs_str = build_context_from_matches(res, max_chunks=3)
    print("\n--- context preview ---\n", docs_str)
else:
    print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

# docs_str ê°€ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. (ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´)
if 'docs_str' not in globals() or not isinstance(docs_str, str) or len(docs_str) == 0:
    print("âš ï¸  docs_strê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    docs_str = "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

prompt = PromptTemplate.from_template("""
[ROLE]
You are a fashion coordinator and product curator. You must follow the rules and output format below, and ground your answers in {context} whenever possible.

[INPUTS]
context: {context}
user query: {user_query}

[TASKS]
Select internally 10 product keywords from the context that best fit the user query, each as "keyword + 1â€“2 key attributes".
Combine the selected keywords to internally design 5 style matches (looks) that fit the query. Each look should have a coherent set of top/bottom/outer/shoes/bagâ€“accessories and be season- and occasion-appropriate.
Pick the best 3 looks among the 5 and return ONLY a JSON array that follows the output format.

[REFERENCE â€” CONSULT (do not copy verbatim; adapt to the query)]
Example product keywords
- Linen jacket (light tone, short-sleeve/tweed options)
- Slacks / cotton pants (straight fit, white/beige tones)
- Skirt (long H-line / A-line mini)
- Blouse & summer knit (neckline/ruffle details)
- Dress (black or color-point, textured fabric)
- Shoes (flats, loafers, slim-toe heels/sandals)
- Belt (â‰ˆ2 cm slim belt, accessory point)
- Tote bag (small structured handbag, light point color)
- Scarf (point item for black dress/blouse looks)
- Jewelry (earrings, neat pieces with some weight)

Example style matches (gist)
- Casual neat: blouse/black inner + denim/cotton pants + light linen/tweed jacket + loafers/flats + slim belt/tote
- Feminine classic: minimal-detail blouse/summer knit + long H-line or A-line mini + short-sleeve jacket/knit + slim-toe heels/sandals + earrings/scarf/tote
- Chic refined: high-neck knit/draped top + black straight slacks + collarless jacket/suit set + slim-toe shoes + mini tote/jewelry
- Modern minimal: solid top (white/black) + white slacks/cream cotton + beige jacket + loafers/mules + scarf/leather tote
- Point-focused: black dress/blouse + H-line skirt/dress + (outer optional) + heeled sandals + scarf/statement earrings

[SCORING & SELECTION]
- Fitness: alignment with season/occasion/exposure/tone in the user query.
- Practicality: substitutable items, styling difficulty, movement/photo situations.
- Harmony: color/material/silhouette balance; avoid excessive details.
- Diversity: best-3 should represent distinct concepts.
- Grounding: prefer combinations/rules mentioned in the context.

[CONSTRAINTS]
- No hallucination: prioritize keywords/looks grounded in the context. If context lacks specifics, backfill with safe, general choices; avoid exaggerated claims or brand mentions.
- Terminology normalization: colors (white/black/beige/light gray), materials (linen/cotton/tweed/knit/leather), fits (straight/wide/H/A) must be consistent.
- No repetition: avoid identical compositions/sentences across the best-3.
- Language: English only (all fields and reasons).
- Do NOT use Markdown, code fences, or comments. Output must be plain JSON only.

[OUTPUT FORMAT â€” JSON ONLY]
Return a JSON array using ONLY the keys below (exactly 3 objects). Keep key order; use double quotes for all keys/strings; no trailing commas.
[
  {{
    "look_name": "string",
    "catg_tops": "string",
    "top_color": "string",
    "top_material": "string",
    "catg_bottoms": "string",
    "bottom_color": "string",
    "bottom_material": "string",
    "reason": ["string", "string"]
  }},
  {{ ... second look ... }},
  {{ ... third look ... }}
]

[PROCESS NOTES]
- The 10 keywords and 5-look design steps are internal only and must NOT be printed.
- Ensure the best-3 have distinct concepts; include at least one context-based justification in each reason (e.g., "Light-tone linen jacket offers neatness and airflow for summer office/guest looks").
- If omitting outerwear is reasonable, state the condition in the reason (e.g., heat vs. indoor A/C).

[FINAL INSTRUCTION]
Follow the rules above and return ONLY the "JSON array (best-3)". Do not include Markdown, code blocks, pre/post text, or any extra content.
""")

llm = ChatOpenAI(model=OPENAI_LLM_MODEL, temperature=0)
output_parser = JsonOutputParser()
chain = prompt | llm | output_parser

question = "ì—¬ë¦„ ì˜¤í”¼ìŠ¤ë£©"
answer = chain.invoke({"user_query": question, "context": docs_str})
print(answer)


