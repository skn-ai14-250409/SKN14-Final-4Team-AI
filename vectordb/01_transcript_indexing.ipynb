{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7220,
     "status": "ok",
     "timestamp": 1756364330132,
     "user": {
      "displayName": "ì´ì„œ",
      "userId": "13175179982189425886"
     },
     "user_tz": -540
    },
    "id": "Tpf2_A5j68tf",
    "outputId": "d55d97a0-9307-4739-fc7b-250e7f286c35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping pinecone-client as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping langchain-pinecone as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "âŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m),   \u001b[33m\"\u001b[39m\u001b[33mâŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPINECONE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mâŒ PINECONE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# ===== 3) ë¡œì»¬ ì´ë¦„ ì¶©ëŒ ë°©ì§€ (./pinecone.py ë“±) =====\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: âŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# ===== 1) ì¶©ëŒ ì œê±° & í•„ìš”í•œ ë²„ì „ ì„¤ì¹˜ =====\n",
    "%pip -q uninstall -y pinecone pinecone-client langchain-pinecone >/dev/null\n",
    "%pip -q install --no-cache-dir \"pinecone[asyncio]>=6,<8\" \"langchain-openai>=0.1.7\" \"langchain>=0.2.14\" \"openai>=1.30.0\" tiktoken >/dev/null\n",
    "\n",
    "# ===== 2) í™˜ê²½ë³€ìˆ˜(Colab userdata ì‚¬ìš© ì‹œ) =====\n",
    "import os, glob\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"OPENAI_API_KEY\"]   = userdata.get(\"FINAL_OPENAI_API_KEY\") or os.getenv(\"OPENAI_API_KEY\",\"\")\n",
    "    os.environ[\"PINECONE_API_KEY\"] = userdata.get(\"PINECONE_API_KEY\") or os.getenv(\"PINECONE_API_KEY\",\"\")\n",
    "    # LangSmith (ì„ íƒ)\n",
    "    for k in [\"LANGSMITH_TRACING\",\"LANGSMITH_ENDPOINT\",\"LANGSMITH_API_KEY\",\"LANGSMITH_PROJECT\"]:\n",
    "        v = userdata.get(k) or os.getenv(k)\n",
    "        if v: os.environ[k] = v\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "assert os.environ.get(\"OPENAI_API_KEY\"),   \"âŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "assert os.environ.get(\"PINECONE_API_KEY\"), \"âŒ PINECONE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# ===== 3) ë¡œì»¬ ì´ë¦„ ì¶©ëŒ ë°©ì§€ (./pinecone.py ë“±) =====\n",
    "_conflicts = [p for p in glob.glob(\"./pinecone*\") if Path(p).exists()]\n",
    "if _conflicts:\n",
    "    raise RuntimeError(f\"âŒ ìž‘ì—… í´ë” ì´ë¦„ ì¶©ëŒ: {_conflicts}  (ì´ë¦„ ë³€ê²½ í›„ ìž¬ì‹¤í–‰)\")\n",
    "\n",
    "# ===== 4) ìž„í¬íŠ¸ =====\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "print(\"OK: imports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1756364332273,
     "user": {
      "displayName": "ì´ì„œ",
      "userId": "13175179982189425886"
     },
     "user_tz": -540
    },
    "id": "A1fChG22qi7V"
   },
   "outputs": [],
   "source": [
    "# ===== ì„¤ì • =====\n",
    "INDEX_NAME   = \"transcripts-test\"\n",
    "NAMESPACE    = \"transcripts\"\n",
    "CLOUD, REGION = \"aws\", \"us-east-1\"\n",
    "METRIC       = \"cosine\"\n",
    "\n",
    "OPENAI_EMBEDDING_MODEL = \"text-embedding-3-small\"  # í•„ìš” ì‹œ \"text-embedding-3-large\"\n",
    "OPENAI_LLM_MODEL       = \"gpt-4o-mini\"             # ë˜ëŠ” \"gpt-4o\"\n",
    "\n",
    "MODEL_DIMS = {\"text-embedding-3-small\": 1536, \"text-embedding-3-large\": 3072}\n",
    "EMBED_DIM  = MODEL_DIMS[OPENAI_EMBEDDING_MODEL]\n",
    "\n",
    "# ===== í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ & ì²­í¬ =====\n",
    "import re, uuid\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    t = re.sub(r\"[ \\t]+\\n\", \"\\n\", t)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t).strip()\n",
    "    t = t.replace(\"â€“\", \"-\").replace(\"â€”\", \"-\")\n",
    "    return t\n",
    "\n",
    "def split_sections(t: str):\n",
    "    parts = re.split(r\"\\n\\s*(\\d\\)\\s*[^\\n]+)\\s*\\n\", t)\n",
    "    docs = []\n",
    "    if len(parts) > 1:\n",
    "        for i in range(1, len(parts), 2):\n",
    "            header = parts[i].strip()\n",
    "            body   = parts[i+1].strip()\n",
    "            section_name = re.sub(r\"^\\d\\)\\s*\", \"\", header).strip()\n",
    "            docs.append({\"section\": section_name, \"text\": body})\n",
    "    else:\n",
    "        docs.append({\"section\": \"General\", \"text\": t})\n",
    "    return docs\n",
    "\n",
    "def chunk_by_paragraph(t, max_chars=1600, overlap_chars=200):\n",
    "    paras = [p.strip() for p in re.split(r\"\\n{2,}\", t) if p.strip()]\n",
    "    chunks, cur = [], \"\"\n",
    "    for p in paras:\n",
    "        if len(cur) + len(p) + 2 <= max_chars:\n",
    "            cur = (cur + \"\\n\\n\" + p).strip()\n",
    "        else:\n",
    "            if cur: chunks.append(cur); cur = p\n",
    "    if cur: chunks.append(cur)\n",
    "    if overlap_chars and len(chunks) > 1:\n",
    "        for k in range(1, len(chunks)):\n",
    "            prefix = chunks[k-1][-overlap_chars:]\n",
    "            chunks[k] = (prefix + \"\\n\" + chunks[k]).strip()\n",
    "    return chunks\n",
    "\n",
    "# ===== occasion ìžë™ íƒœê¹… =====\n",
    "def infer_occasion(text: str):\n",
    "    t = text.lower()\n",
    "    tags = set()\n",
    "    if re.search(r\"\\b(office|work|commute|promotion|slacks|shirt|tweed|mary jane)\\b\", t):\n",
    "        tags.add(\"work\")\n",
    "    if re.search(r\"\\b(festival|concert)\\b\", t):\n",
    "        tags.add(\"festival\")\n",
    "    if re.search(r\"\\b(travel|airport)\\b\", t):\n",
    "        tags.add(\"travel\")\n",
    "    if re.search(r\"\\b(casual|weekend|hobo bag)\\b\", t):\n",
    "        tags.add(\"casual\")\n",
    "    return list(tags or {\"general\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOO4raBRrHV5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7907,
     "status": "ok",
     "timestamp": 1756364343097,
     "user": {
      "displayName": "ì´ì„œ",
      "userId": "13175179982189425886"
     },
     "user_tz": -540
    },
    "id": "u4zdu0Yf7L1G",
    "outputId": "c20376f4-e1fd-46f0-a0b8-abb2aa3ded2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created index: transcripts-test\n",
      "OK: Pinecone & Embeddings ready\n"
     ]
    }
   ],
   "source": [
    "# ===== Pinecone ì´ˆê¸°í™” & ì¸ë±ìŠ¤ ìƒì„±/ì ‘ì† =====\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "if INDEX_NAME not in {i.name for i in pc.list_indexes()}:\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME, dimension=EMBED_DIM, metric=METRIC,\n",
    "        spec=ServerlessSpec(cloud=CLOUD, region=REGION),\n",
    "    )\n",
    "    print(f\"âœ… Created index: {INDEX_NAME}\")\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# ===== ìž„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ =====\n",
    "emb = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL)\n",
    "print(\"OK: Pinecone & Embeddings ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1756364345491,
     "user": {
      "displayName": "ì´ì„œ",
      "userId": "13175179982189425886"
     },
     "user_tz": -540
    },
    "id": "theVRUnJ8AQX"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def ingest_files(file_paths, *, namespace=NAMESPACE, add_snippet=True):\n",
    "    total_vectors, doc_map = 0, {}\n",
    "    for f in file_paths:\n",
    "        path = Path(f)\n",
    "        assert path.exists(), f\"âŒ íŒŒì¼ ì—†ìŒ: {path.resolve()}\"\n",
    "\n",
    "        text = clean_text(path.read_text(encoding=\"utf-8\"))\n",
    "        docs = split_sections(text)\n",
    "\n",
    "        # ë ˆì½”ë“œ êµ¬ì„±(ìŠ¤ë‹ˆíŽ«/occasion ë©”íƒ€ í¬í•¨)\n",
    "        records = []\n",
    "        for d in docs:\n",
    "            for ch in chunk_by_paragraph(d[\"text\"], max_chars=1600, overlap_chars=200):\n",
    "                md = {\n",
    "                    \"section\": d[\"section\"],\n",
    "                    \"season\": \"summer\",\n",
    "                    \"exposure\": \"non_revealing\",\n",
    "                    \"source\": str(path),\n",
    "                    \"occasion\": infer_occasion(ch)\n",
    "                }\n",
    "                if add_snippet:\n",
    "                    md[\"snippet\"] = ch[:220].replace(\"\\n\", \" \")\n",
    "                records.append({\"text\": f\"Section: {d['section']}\\n\\n{ch}\", \"metadata\": md})\n",
    "\n",
    "        # ì—…ì„œíŠ¸\n",
    "        if not records:\n",
    "            continue\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        texts  = [r[\"text\"] for r in records]\n",
    "        metas  = [r[\"metadata\"] for r in records]\n",
    "        ids    = [f\"{doc_id}-{i}\" for i in range(len(records))]\n",
    "\n",
    "        vectors = []\n",
    "        BATCH = 64\n",
    "        for s in range(0, len(texts), BATCH):\n",
    "            embs = emb.embed_documents(texts[s:s+BATCH])\n",
    "            for j, vec in enumerate(embs):\n",
    "                k = s + j\n",
    "                vectors.append({\n",
    "                    \"id\": ids[k],\n",
    "                    \"values\": vec,\n",
    "                    \"metadata\": {**metas[k], \"doc_id\": doc_id, \"chunk_id\": k, \"lang\": \"en\"},\n",
    "                })\n",
    "\n",
    "        index.upsert(vectors=vectors, namespace=namespace)\n",
    "        total_vectors += len(vectors)\n",
    "        doc_map[doc_id] = str(path.name)\n",
    "        print(f\"âœ… Upserted {len(vectors)} vectors from '{path.name}' (ns='{namespace}')\")\n",
    "    print(f\"ðŸŽ¯ Done. total_vectors={total_vectors}, files={len(file_paths)}, namespace='{namespace}'\")\n",
    "    return {\"total_vectors\": total_vectors, \"doc_map\": doc_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1889,
     "status": "ok",
     "timestamp": 1756364368893,
     "user": {
      "displayName": "ì´ì„œ",
      "userId": "13175179982189425886"
     },
     "user_tz": -540
    },
    "id": "UerDAruk8BRu",
    "outputId": "0bb53a4e-4078-4b61-f786-ff22b5f07325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Upserted 8 vectors from 'transcript_sample.txt' (ns='transcripts')\n",
      "âœ… Upserted 8 vectors from 'transcript_sample2.txt' (ns='transcripts')\n",
      "ðŸŽ¯ Done. total_vectors=16, files=2, namespace='transcripts'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_vectors': 16,\n",
       " 'doc_map': {'763da065-c7aa-4369-92cc-15df778fad3b': 'transcript_sample.txt',\n",
       "  '59bd4f03-ae4b-441d-9a6b-766abeaed1e4': 'transcript_sample2.txt'}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ingest_files([\"transcript_sample.txt\", \"transcript_sample2.txt\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1756364370390,
     "user": {
      "displayName": "ì´ì„œ",
      "userId": "13175179982189425886"
     },
     "user_tz": -540
    },
    "id": "JLIri-bT8Gob",
    "outputId": "dfbfb04a-8d2f-433b-88ca-0c02ee0452ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] score=0.3157  section=General  occasion=['work'] source=transcript_sample2.txt\n",
      "snippet: legant vibe. Those big shoulder â€œwork bagsâ€ can weigh the look down. You donâ€™t need new clothes or items-if you mind just a few **key points**, youâ€™ll look like the best-dressed guest after the bride. How about that? Sum\n",
      "\n",
      "[2] score=0.3157  section=General  occasion=['work'] source=transcript_sample2.txt\n",
      "snippet: legant vibe. Those big shoulder â€œwork bagsâ€ can weigh the look down. You donâ€™t need new clothes or items-if you mind just a few **key points**, youâ€™ll look like the best-dressed guest after the bride. How about that? Sum\n",
      "\n",
      "[3] score=0.2985  section=Pure & Clean  occasion=['work', 'casual', 'festival'] source=transcript_sample.txt\n",
      "snippet: ace fabric bag. Donâ€™t stuff it-part of the charm is just carrying it as is. Same outfit, but change the shoes and bag and itâ€™s totally different-honestly, you can even get a hip look out of this base. Thinking summer fes\n",
      "\n",
      "[4] score=0.2985  section=Pure & Clean  occasion=['work', 'casual', 'festival'] source=transcript_sample.txt\n",
      "snippet: ace fabric bag. Donâ€™t stuff it-part of the charm is just carrying it as is. Same outfit, but change the shoes and bag and itâ€™s totally different-honestly, you can even get a hip look out of this base. Thinking summer fes\n",
      "\n",
      "[5] score=0.2822  section=Bright, Lively & Cute (without showing skin)  occasion=['work'] source=transcript_sample.txt\n",
      "snippet: r work.  This particular piece has black stitching and black buttons, so you might think, â€œShouldnâ€™t the belt be black?â€ If you want to soften it, go with brown instead. Shoes-keep them simple basics. This design is swee\n",
      "\n",
      "--- context preview ---\n",
      " [General] legant vibe. Those big shoulder â€œwork bagsâ€ can weigh the look down. You donâ€™t need new clothes or items-if you mind just a few **key points**, youâ€™ll look like the best-dressed guest after the bride. How about that? Sum\n",
      "\n",
      "[General] legant vibe. Those big shoulder â€œwork bagsâ€ can weigh the look down. You donâ€™t need new clothes or items-if you mind just a few **key points**, youâ€™ll look like the best-dressed guest after the bride. How about that? Sum\n",
      "\n",
      "[Pure & Clean] ace fabric bag. Donâ€™t stuff it-part of the charm is just carrying it as is. Same outfit, but change the shoes and bag and itâ€™s totally different-honestly, you can even get a hip look out of this base. Thinking summer fes\n"
     ]
    }
   ],
   "source": [
    "def pinecone_search(query_text, k=5, season=\"summer\", occasion=None, section=None, namespace=NAMESPACE):\n",
    "    qvec = emb.embed_query(query_text)\n",
    "    _filter = {}\n",
    "    if season:   _filter[\"season\"] = {\"$eq\": season}\n",
    "    if occasion: _filter[\"occasion\"] = {\"$eq\": occasion}\n",
    "    if section:  _filter[\"section\"] = {\"$eq\": section}\n",
    "\n",
    "    res = index.query(\n",
    "        vector=qvec, top_k=k, include_values=False, include_metadata=True,\n",
    "        namespace=namespace, filter=_filter or None\n",
    "    )\n",
    "    matches = res.get(\"matches\", []) or []\n",
    "    for i, m in enumerate(matches, 1):\n",
    "        md = m.get(\"metadata\", {}) or {}\n",
    "        print(f\"\\n[{i}] score={m.get('score'):.4f}  section={md.get('section')}  occasion={md.get('occasion')} source={md.get('source')}\")\n",
    "        if \"snippet\" in md:\n",
    "            print(\"snippet:\", md[\"snippet\"])\n",
    "    return res\n",
    "\n",
    "def build_context_from_matches(res, max_chunks=3, max_chars=800):\n",
    "    chunks = []\n",
    "    for m in (res.get(\"matches\") or [])[:max_chunks]:\n",
    "        md = m.get(\"metadata\", {}) or {}\n",
    "        body = md.get(\"snippet\",\"\")\n",
    "        title = md.get(\"section\",\"\")\n",
    "        chunks.append(f\"[{title}] {body[:max_chars]}\")\n",
    "    return \"\\n\\n\".join(chunks)\n",
    "\n",
    "# ì˜ˆì‹œ: ì—¬ë¦„ 'ì¶œê·¼ë£©'ë§Œ ë³´ê¸°\n",
    "res = pinecone_search(\"30ëŒ€ ì—¬ìž ì—¬ë¦„ ì¶œê·¼ë£©\", k=5, occasion=\"work\")\n",
    "docs_str = build_context_from_matches(res, max_chunks=3)\n",
    "print(\"\\n--- context preview ---\\n\", docs_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7023,
     "status": "ok",
     "timestamp": 1756364387240,
     "user": {
      "displayName": "ì´ì„œ",
      "userId": "13175179982189425886"
     },
     "user_tz": -540
    },
    "id": "yS1qyGhl87pg",
    "outputId": "0f62d8d1-3a4b-4ef1-c61f-73e96da41fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'look_name': 'Chic Minimalist', 'catg_tops': 'blouse', 'top_color': 'white', 'top_material': 'cotton', 'catg_bottoms': 'slacks', 'bottom_color': 'beige', 'bottom_material': 'cotton', 'reason': ['The white cotton blouse provides a fresh, clean look suitable for summer office wear.', 'Beige slacks maintain a light tone, ensuring comfort and elegance.']}, {'look_name': 'Feminine Classic', 'catg_tops': 'summer knit', 'top_color': 'light gray', 'top_material': 'knit', 'catg_bottoms': 'A-line skirt', 'bottom_color': 'white', 'bottom_material': 'cotton', 'reason': ['Light gray summer knit adds a soft touch, perfect for warm weather.', 'The white A-line skirt enhances femininity while keeping the outfit airy.']}, {'look_name': 'Modern Professional', 'catg_tops': 'high-neck top', 'top_color': 'black', 'top_material': 'knit', 'catg_bottoms': 'straight slacks', 'bottom_color': 'light gray', 'bottom_material': 'cotton', 'reason': ['A black high-neck top offers sophistication and a polished appearance for office settings.', 'Light gray straight slacks provide a modern contrast, ensuring a balanced silhouette.']}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# docs_str ê°€ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤. (ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìžì—´)\n",
    "assert 'docs_str' in globals() and isinstance(docs_str, str) and len(docs_str) > 0, \"docs_str ê°€ ë¹„ì–´ìžˆê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "[ROLE]\n",
    "You are a fashion coordinator and product curator. You must follow the rules and output format below, and ground your answers in {context} whenever possible.\n",
    "\n",
    "[INPUTS]\n",
    "context: {context}\n",
    "user query: {user_query}\n",
    "\n",
    "[TASKS]\n",
    "Select internally 10 product keywords from the context that best fit the user query, each as \"keyword + 1â€“2 key attributes\".\n",
    "Combine the selected keywords to internally design 5 style matches (looks) that fit the query. Each look should have a coherent set of top/bottom/outer/shoes/bagâ€“accessories and be season- and occasion-appropriate.\n",
    "Pick the best 3 looks among the 5 and return ONLY a JSON array that follows the output format.\n",
    "\n",
    "[REFERENCE â€” CONSULT (do not copy verbatim; adapt to the query)]\n",
    "Example product keywords\n",
    "- Linen jacket (light tone, short-sleeve/tweed options)\n",
    "- Slacks / cotton pants (straight fit, white/beige tones)\n",
    "- Skirt (long H-line / A-line mini)\n",
    "- Blouse & summer knit (neckline/ruffle details)\n",
    "- Dress (black or color-point, textured fabric)\n",
    "- Shoes (flats, loafers, slim-toe heels/sandals)\n",
    "- Belt (â‰ˆ2 cm slim belt, accessory point)\n",
    "- Tote bag (small structured handbag, light point color)\n",
    "- Scarf (point item for black dress/blouse looks)\n",
    "- Jewelry (earrings, neat pieces with some weight)\n",
    "\n",
    "Example style matches (gist)\n",
    "- Casual neat: blouse/black inner + denim/cotton pants + light linen/tweed jacket + loafers/flats + slim belt/tote\n",
    "- Feminine classic: minimal-detail blouse/summer knit + long H-line or A-line mini + short-sleeve jacket/knit + slim-toe heels/sandals + earrings/scarf/tote\n",
    "- Chic refined: high-neck knit/draped top + black straight slacks + collarless jacket/suit set + slim-toe shoes + mini tote/jewelry\n",
    "- Modern minimal: solid top (white/black) + white slacks/cream cotton + beige jacket + loafers/mules + scarf/leather tote\n",
    "- Point-focused: black dress/blouse + H-line skirt/dress + (outer optional) + heeled sandals + scarf/statement earrings\n",
    "\n",
    "[SCORING & SELECTION]\n",
    "- Fitness: alignment with season/occasion/exposure/tone in the user query.\n",
    "- Practicality: substitutable items, styling difficulty, movement/photo situations.\n",
    "- Harmony: color/material/silhouette balance; avoid excessive details.\n",
    "- Diversity: best-3 should represent distinct concepts.\n",
    "- Grounding: prefer combinations/rules mentioned in the context.\n",
    "\n",
    "[CONSTRAINTS]\n",
    "- No hallucination: prioritize keywords/looks grounded in the context. If context lacks specifics, backfill with safe, general choices; avoid exaggerated claims or brand mentions.\n",
    "- Terminology normalization: colors (white/black/beige/light gray), materials (linen/cotton/tweed/knit/leather), fits (straight/wide/H/A) must be consistent.\n",
    "- No repetition: avoid identical compositions/sentences across the best-3.\n",
    "- Language: English only (all fields and reasons).\n",
    "- Do NOT use Markdown, code fences, or comments. Output must be plain JSON only.\n",
    "\n",
    "[OUTPUT FORMAT â€” JSON ONLY]\n",
    "Return a JSON array using ONLY the keys below (exactly 3 objects). Keep key order; use double quotes for all keys/strings; no trailing commas.\n",
    "[\n",
    "  {{\n",
    "    \"look_name\": \"string\",\n",
    "    \"catg_tops\": \"string\",\n",
    "    \"top_color\": \"string\",\n",
    "    \"top_material\": \"string\",\n",
    "    \"catg_bottoms\": \"string\",\n",
    "    \"bottom_color\": \"string\",\n",
    "    \"bottom_material\": \"string\",\n",
    "    \"reason\": [\"string\", \"string\"]\n",
    "  }},\n",
    "  {{ ... second look ... }},\n",
    "  {{ ... third look ... }}\n",
    "]\n",
    "\n",
    "[PROCESS NOTES]\n",
    "- The 10 keywords and 5-look design steps are internal only and must NOT be printed.\n",
    "- Ensure the best-3 have distinct concepts; include at least one context-based justification in each reason (e.g., \"Light-tone linen jacket offers neatness and airflow for summer office/guest looks\").\n",
    "- If omitting outerwear is reasonable, state the condition in the reason (e.g., heat vs. indoor A/C).\n",
    "\n",
    "[FINAL INSTRUCTION]\n",
    "Follow the rules above and return ONLY the \"JSON array (best-3)\". Do not include Markdown, code blocks, pre/post text, or any extra content.\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(model=OPENAI_LLM_MODEL, temperature=0)\n",
    "output_parser = JsonOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "question = \"summer office work outfit\"\n",
    "answer = chain.invoke({\"user_query\": question, \"context\": docs_str})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSNiqpevIqTL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP3EeolmUpbRvDmbJlqZI2O",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "skn14_final_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
